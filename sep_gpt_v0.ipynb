{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPg32Yrsh3SO5ZVqY0GCChb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gedman4b/AI/blob/main/sep_gpt_v0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "jupyter_config_dir = os.path.expanduser('~/.jupyter')\n",
        "os.makedirs(jupyter_config_dir, exist_ok=True)\n",
        "\n",
        "config_path = os.path.join(jupyter_config_dir, 'jupyter_notebook_config.py')\n",
        "with open(config_path, 'a') as f:\n",
        "    f.write(\"c.FileContentsManager.save_checkpoints = False\\n\")\n",
        "\n",
        "print(f\"Added 'c.FileContentsManager.save_checkpoints = False' to {config_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz_PMVRGdcMt",
        "outputId": "b6d54b77-a2ca-4f89-fe62-ec12d56b3da1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 'c.FileContentsManager.save_checkpoints = False' to /root/.jupyter/jupyter_notebook_config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xwdCVUSiL8R7"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import math\n",
        "import csv\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import functools\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    import torchaudio\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"torchaudio is required. Install with: pip install torchaudio\") from e\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Repro / utils\n",
        "# -------------------------\n",
        "def set_seed(seed: int = 1337):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def ensure_dir(p: str):\n",
        "    os.makedirs(p, exist_ok=True)\n",
        "\n",
        "\n",
        "def human_time(seconds: float) -> str:\n",
        "    if seconds < 60:\n",
        "        return f\"{seconds:.1f}s\"\n",
        "    if seconds < 3600:\n",
        "        return f\"{seconds/60:.1f}m\"\n",
        "    return f\"{seconds/3600:.1f}h\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Audio + tokens\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class TokenSpec:\n",
        "    # 8-bit magnitude tokens\n",
        "    n_mag_tokens: int = 256  # 0..255\n",
        "\n",
        "    # Special tokens (offset above mag tokens)\n",
        "    BOS: int = 256\n",
        "    MIX: int = 257\n",
        "    mix: int = 258\n",
        "    SEP: int = 259\n",
        "    STEM: int = 260\n",
        "    target: int = 261\n",
        "    EOS: int = 262\n",
        "\n",
        "    @property\n",
        "    def n_special(self) -> int:\n",
        "        return 7\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return self.n_mag_tokens + self.n_special\n",
        "\n",
        "    def stem_token(self, stem_index: int) -> int:\n",
        "        # encode stem id as a single token above specials\n",
        "        # (keeps file simple; supports up to a few dozen stems)\n",
        "        return self.vocab_size + stem_index\n",
        "\n",
        "    def total_vocab(self, n_stems: int) -> int:\n",
        "        return self.vocab_size + n_stems\n",
        "\n",
        "\n",
        "class MelReducer:\n",
        "    \"\"\"\n",
        "    Converts linear-frequency magnitude (n_fft//2+1) <-> mel (n_mels)\n",
        "    via a fixed mel filterbank matrix and pseudo-inverse.\n",
        "\n",
        "    We use it on magnitudes (not power), and keep it simple.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, sample_rate: int, n_fft: int, n_mels: int = 64, f_min: float = 0.0, f_max: Optional[float] = None, device=\"cpu\"\n",
        "    ):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_fft = n_fft\n",
        "        self.n_mels = n_mels\n",
        "        self.f_min = f_min\n",
        "        self.f_max = f_max if f_max is not None else sample_rate / 2\n",
        "        self.device = device\n",
        "\n",
        "        n_freqs = n_fft // 2 + 1\n",
        "\n",
        "        # Create mel filterbank. Torchaudio usually returns (n_mels, n_freqs).\n",
        "        fb_raw = torchaudio.functional.melscale_fbanks(\n",
        "            n_freqs=n_freqs,\n",
        "            f_min=self.f_min,\n",
        "            f_max=self.f_max,\n",
        "            n_mels=self.n_mels,\n",
        "            sample_rate=self.sample_rate,\n",
        "            norm=\"slaney\",\n",
        "            mel_scale=\"htk\",\n",
        "        )\n",
        "        fb_raw = fb_raw.to(torch.float32).to(device)\n",
        "\n",
        "        # Robustly ensure mel_forward_matrix is (n_freqs, n_mels) i.e. (129, 16)\n",
        "        if fb_raw.shape[0] == self.n_mels:\n",
        "            # shape is (n_mels, n_freqs) -> transpose to (n_freqs, n_mels)\n",
        "            self.mel_forward_matrix = fb_raw.t()\n",
        "        else:\n",
        "            # assume shape is already (n_freqs, n_mels)\n",
        "            self.mel_forward_matrix = fb_raw\n",
        "\n",
        "        # Pseudo-inverse: (n_mels, n_freqs)\n",
        "        self.mel_inverse_matrix = torch.linalg.pinv(self.mel_forward_matrix)\n",
        "\n",
        "    def linear_to_mel(self, mag_lin: torch.Tensor) -> torch.Tensor:\n",
        "        # mag_lin: (..., n_freqs)\n",
        "        # self.mel_forward_matrix: (n_freqs, n_mels)\n",
        "        # Result: (..., n_mels)\n",
        "        return torch.matmul(mag_lin, self.mel_forward_matrix)\n",
        "\n",
        "    def mel_to_linear(self, mag_mel: torch.Tensor) -> torch.Tensor:\n",
        "        # mag_mel: (..., n_mels)\n",
        "        # self.mel_inverse_matrix: (n_mels, n_freqs)\n",
        "        # Result: (..., n_freqs)\n",
        "        return torch.matmul(mag_mel, self.mel_inverse_matrix)\n",
        "\n",
        "\n",
        "class AudioTokenizer:\n",
        "    \"\"\"\n",
        "    Tokenize waveform -> STFT magnitude -> log1p -> mel -> quantize (0..255)\n",
        "    Also supports inverse: tokens -> mel -> linear -> ISTFT with mixture phase.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        sample_rate: int = 44100,\n",
        "        n_fft: int = 256,\n",
        "        hop: int = 256,\n",
        "        n_mels: int = 64,\n",
        "        token_spec: Optional[TokenSpec] = None,\n",
        "        device: str = \"cpu\",\n",
        "        log_eps: float = 1e-8,\n",
        "    ):\n",
        "        self.sr = sample_rate\n",
        "        self.n_fft = n_fft\n",
        "        self.hop = hop\n",
        "        self.win_length = n_fft  # rectangular window equivalent\n",
        "        self.window = torch.ones(self.win_length, device=device)  # rectangular\n",
        "        self.n_mels = n_mels\n",
        "        self.spec = token_spec or TokenSpec()\n",
        "        self.device = device\n",
        "        self.log_eps = log_eps\n",
        "        self.reducer = MelReducer(sample_rate, n_fft, n_mels=n_mels, device=device)\n",
        "\n",
        "        # Quantization calibration (global fixed baseline)\n",
        "        # log1p magnitudes tend to live in [0, ~something]. We clamp to a range.\n",
        "        # These can be tuned; baseline values work OK across many music datasets.\n",
        "        self.q_min = 0.0\n",
        "        self.q_max = 6.0  # log1p(mag) clamp upper bound\n",
        "\n",
        "    def stft(self, wav: torch.Tensor) -> torch.Tensor:\n",
        "        # wav: (T,)\n",
        "        return torch.stft(\n",
        "            wav,\n",
        "            n_fft=self.n_fft,\n",
        "            hop_length=self.hop,\n",
        "            win_length=self.win_length,\n",
        "            window=self.window,\n",
        "            center=False,\n",
        "            return_complex=True,\n",
        "        )\n",
        "\n",
        "    def istft(self, stft_c: torch.Tensor, length: int) -> torch.Tensor:\n",
        "        return torch.istft(\n",
        "            stft_c,\n",
        "            n_fft=self.n_fft,\n",
        "            hop_length=self.hop,\n",
        "            win_length=self.win_length,\n",
        "            window=self.window,\n",
        "            center=False,\n",
        "            length=length,\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def wav_to_tokens(self, wav: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "          tokens: (frames, n_mels) int64 in [0..255]\n",
        "          mix_phase: complex phase from STFT (frames, freqs)\n",
        "        \"\"\"\n",
        "        wav = wav.to(self.device).to(torch.float32)\n",
        "        X = self.stft(wav)  # (freqs, frames) complex\n",
        "        X = X.transpose(0, 1).contiguous()  # (frames, freqs)\n",
        "        mag = torch.abs(X).clamp_min(self.log_eps)  # (frames, freqs)\n",
        "        phase = X / mag  # unit complex, (frames, freqs)\n",
        "\n",
        "        # linear -> mel on magnitude\n",
        "        mag_mel = self.reducer.linear_to_mel(mag)  # (frames, mels)\n",
        "        logmag = torch.log1p(mag_mel)  # (frames, mels)\n",
        "\n",
        "        # quantize to 0..255\n",
        "        q = (logmag.clamp(self.q_min, self.q_max) - self.q_min) / (self.q_max - self.q_min)\n",
        "        tokens = torch.round(q * (self.spec.n_mag_tokens - 1)).to(torch.int64)\n",
        "        return tokens, phase\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def tokens_to_wav_with_phase(self, tokens: torch.Tensor, phase: torch.Tensor, length: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        tokens: (frames, mels) int64 0..255\n",
        "        phase: (frames, freqs) unit complex (from mixture)\n",
        "        \"\"\"\n",
        "        tokens = tokens.to(self.device)\n",
        "        # dequantize\n",
        "        q = tokens.to(torch.float32) / (self.spec.n_mag_tokens - 1)\n",
        "        logmag = q * (self.q_max - self.q_min) + self.q_min\n",
        "        mag_mel = torch.expm1(logmag).clamp_min(0.0)  # (frames, mels)\n",
        "\n",
        "        mag_lin = self.reducer.mel_to_linear(mag_mel)  # (frames, freqs)\n",
        "        mag_lin = mag_lin.clamp_min(self.log_eps)\n",
        "        X = (mag_lin * phase).transpose(0, 1).contiguous()  # (freqs, frames)\n",
        "        wav = self.istft(X, length=length)\n",
        "        return wav"
      ],
      "metadata": {
        "id": "St2_3m0jM93W"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Model: decoder-only GPT\n",
        "# -------------------------\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, dropout: float):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads\n",
        "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
        "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        # registered buffer for causal mask will be created dynamically by sequence length\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x)  # (B, T, 3C)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        q = q.view(B, T, self.n_heads, self.d_head).transpose(1, 2)  # (B, H, T, Dh)\n",
        "        k = k.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.d_head)  # (B, H, T, T)\n",
        "        # causal mask\n",
        "        mask = torch.triu(torch.ones(T, T, device=x.device, dtype=torch.bool), diagonal=1)\n",
        "        att = att.masked_fill(mask, float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.drop(att)\n",
        "\n",
        "        y = att @ v  # (B, H, T, Dh)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.proj(y)\n",
        "        y = self.drop(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, d_model: int, dropout: float, mult: int = 4):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, mult * d_model)\n",
        "        self.fc2 = nn.Linear(mult * d_model, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = CausalSelfAttention(d_model, n_heads, dropout)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.mlp = MLP(d_model, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTSeparator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 512,\n",
        "        n_layers: int = 8,\n",
        "        n_heads: int = 8,\n",
        "        dropout: float = 0.1,\n",
        "        max_seq_len: int = 4096,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([Block(d_model, n_heads, dropout) for _ in range(n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Embedding)):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
        "        # idx: (B, T)\n",
        "        B, T = idx.shape\n",
        "        if T > self.max_seq_len:\n",
        "            raise ValueError(f\"Sequence length {T} exceeds max_seq_len {self.max_seq_len}\")\n",
        "\n",
        "        pos = torch.arange(0, T, device=idx.device, dtype=torch.long).unsqueeze(0)\n",
        "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
        "        x = self.drop(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)  # (B, T, vocab)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "hKtiY425N5UJ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "def list_tracks(dataset_root: str) -> List[str]:\n",
        "    tracks = []\n",
        "    for name in sorted(os.listdir(dataset_root)):\n",
        "        if name.startswith('.'):  # Ignore hidden directories like .ipynb_checkpoints\n",
        "            continue\n",
        "        p = os.path.join(dataset_root, name)\n",
        "        if os.path.isdir(p):\n",
        "            tracks.append(p)\n",
        "    return tracks\n",
        "\n",
        "\n",
        "def load_wav_mono(path: str, target_sr: int) -> torch.Tensor:\n",
        "    wav, sr = torchaudio.load(path)  # (C, T)\n",
        "    if sr != target_sr:\n",
        "        wav = torchaudio.functional.resample(wav, sr, target_sr)\n",
        "    wav = wav.mean(dim=0)  # mono\n",
        "    return wav\n",
        "\n",
        "\n",
        "def stems_in_track(track_dir: str) -> List[str]:\n",
        "    # all .wav except mixture.wav\n",
        "    wavs = [f for f in os.listdir(track_dir) if f.lower().endswith(\".wav\")]\n",
        "    stems = [f for f in wavs if f.lower() != \"mixture.wav\"]\n",
        "    stems.sort()\n",
        "    return stems"
      ],
      "metadata": {
        "id": "AeyNIxlWOZye"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StemSeparationDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Produces windowed examples:\n",
        "      - mixture waveform window\n",
        "      - target stem waveform window\n",
        "    Tokenization happens in the collate_fn for speed batching (or in __getitem__ if desired).\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_root: str,\n",
        "        sample_rate: int = 44100,\n",
        "        window_sec: float = 0.1,\n",
        "        mix_from_stems: bool = True,\n",
        "        explicit_tracks: Optional[List[str]] = None,\n",
        "        stem_names: Optional[List[str]] = None,\n",
        "        seed: int = 1337,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dataset_root = dataset_root\n",
        "        self.sr = sample_rate\n",
        "        self.window_sec = window_sec\n",
        "        self.win_len = int(round(window_sec * sample_rate))\n",
        "        self.mix_from_stems = mix_from_stems\n",
        "        self.rng = random.Random(seed)\n",
        "\n",
        "        self.tracks = explicit_tracks if explicit_tracks is not None else list_tracks(dataset_root)\n",
        "        if len(self.tracks) == 0:\n",
        "            raise ValueError(f\"No tracks found in {dataset_root}\")\n",
        "\n",
        "        # Determine stems universe\n",
        "        # If stem_names provided, we enforce that; otherwise derive from first track and keep intersection across tracks.\n",
        "        if stem_names is None:\n",
        "            first = stems_in_track(self.tracks[0])\n",
        "            if len(first) == 0:\n",
        "                raise ValueError(f\"No stems found in {self.tracks[0]}\")\n",
        "            self.stem_names = first\n",
        "        else:\n",
        "            self.stem_names = list(stem_names)\n",
        "\n",
        "        # Precompute lengths and valid start positions per track (at least one window)\n",
        "        self.track_meta = []\n",
        "        for td in self.tracks:\n",
        "            mix_path = os.path.join(td, \"mixture.wav\")\n",
        "            if not os.path.exists(mix_path) and not self.mix_from_stems:\n",
        "                continue\n",
        "\n",
        "            # choose a reference stem file for length (or mixture)\n",
        "            ref_path = mix_path if os.path.exists(mix_path) else os.path.join(td, self.stem_names[0])\n",
        "            if not os.path.exists(ref_path):\n",
        "                continue\n",
        "            wav = load_wav_mono(ref_path, self.sr)\n",
        "            T = wav.shape[0]\n",
        "            if T < self.win_len:\n",
        "                continue\n",
        "            self.track_meta.append((td, T))\n",
        "        if len(self.track_meta) == 0:\n",
        "            raise ValueError(\"No usable tracks (check mixture.wav presence or enable mix_from_stems=True).\")\n",
        "\n",
        "        # Define dataset length as a large number; windows sampled randomly for each item\n",
        "        # This avoids epoch-definition confusion and improves mixing.\n",
        "        self.virtual_len = 200_000  # baseline; training is step-based\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.virtual_len\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # Sample a random track and random stem and random window start\n",
        "        td, T = self.rng.choice(self.track_meta)\n",
        "        stem_name = self.rng.choice(self.stem_names)\n",
        "        stem_path = os.path.join(td, stem_name)\n",
        "\n",
        "        # choose start\n",
        "        start = self.rng.randint(0, T - self.win_len)\n",
        "        end = start + self.win_len\n",
        "\n",
        "        # load target stem window\n",
        "        y = load_wav_mono(stem_path, self.sr)[start:end]\n",
        "\n",
        "        # mixture window\n",
        "        mix_path = os.path.join(td, \"mixture.wav\")\n",
        "        if os.path.exists(mix_path):\n",
        "            x = load_wav_mono(mix_path, self.sr)[start:end]\n",
        "        else:\n",
        "            # synthesize mixture from all available stems (intersection)\n",
        "            xs = []\n",
        "            for s in self.stem_names:\n",
        "                p = os.path.join(td, s)\n",
        "                if os.path.exists(p):\n",
        "                    xs.append(load_wav_mono(p, self.sr)[start:end])\n",
        "            if len(xs) == 0:\n",
        "                x = y.clone()\n",
        "            else:\n",
        "                x = torch.stack(xs, dim=0).sum(dim=0)\n",
        "\n",
        "        # simple gain normalization (baseline)\n",
        "        x = x / (x.abs().max().clamp_min(1e-6))\n",
        "        y = y / (y.abs().max().clamp_min(1e-6))\n",
        "\n",
        "        return {\n",
        "            \"track\": os.path.basename(td),\n",
        "            \"stem\": stem_name,\n",
        "            \"stem_index\": self.stem_names.index(stem_name),\n",
        "            \"mixture_wav\": x,\n",
        "            \"target_wav\": y,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "AjHRtzVCOrVv"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_sequence_tokens(\n",
        "    spec: TokenSpec,\n",
        "    mix_tokens_2d: torch.Tensor,     # (F, M) with values 0..255\n",
        "    tgt_tokens_2d: torch.Tensor,     # (F, M) with values 0..255\n",
        "    stem_index: int,\n",
        "    n_stems: int,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Flatten 2D tokens to 1D and build [context + target] sequence.\n",
        "    Returns:\n",
        "      seq: (T,) int64 full input sequence including EOS\n",
        "      target_mask: (T,) bool mask where loss should be applied\n",
        "    \"\"\"\n",
        "    # Determine device from inputs\n",
        "    device = mix_tokens_2d.device\n",
        "\n",
        "    mix_flat = mix_tokens_2d.reshape(-1).to(torch.int64)\n",
        "    tgt_flat = tgt_tokens_2d.reshape(-1).to(torch.int64)\n",
        "\n",
        "    stem_tok = spec.stem_token(stem_index)  # distinct token\n",
        "    vocab_total = spec.total_vocab(n_stems)\n",
        "    if stem_tok >= vocab_total:\n",
        "        raise ValueError(\"stem token out of range; increase n_stems or adjust token mapping\")\n",
        "\n",
        "    seq = torch.cat([\n",
        "        torch.tensor([spec.BOS, spec.MIX, spec.mix], dtype=torch.int64, device=device),\n",
        "        mix_flat,\n",
        "        torch.tensor([spec.SEP, spec.STEM, stem_tok, spec.target], dtype=torch.int64, device=device),\n",
        "        tgt_flat,\n",
        "        torch.tensor([spec.EOS], dtype=torch.int64, device=device),\n",
        "    ], dim=0)\n",
        "\n",
        "    # Loss only on target tokens (tgt_flat positions)\n",
        "    mask = torch.zeros_like(seq, dtype=torch.bool)\n",
        "    tgt_start = 3 + mix_flat.numel() + 4  # after BOS MIX mix + mix_flat + SEP STEM stem_tok target\n",
        "    tgt_end = tgt_start + tgt_flat.numel()\n",
        "    mask[tgt_start:tgt_end] = True\n",
        "    return seq, mask\n",
        "\n",
        "\n",
        "def collate_batch(\n",
        "    batch: List[dict],\n",
        "    tokenizer: AudioTokenizer,\n",
        "    spec: TokenSpec,\n",
        "    n_stems: int,\n",
        "    max_len: int,\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    # tokenize each item and build sequence; then pad\n",
        "    seqs = []\n",
        "    masks = []\n",
        "    meta = []\n",
        "    for item in batch:\n",
        "        mix_tok2d, mix_phase = tokenizer.wav_to_tokens(item[\"mixture_wav\"])\n",
        "        tgt_tok2d, _ = tokenizer.wav_to_tokens(item[\"target_wav\"])\n",
        "        seq, mask = build_sequence_tokens(\n",
        "            spec, mix_tok2d, tgt_tok2d, item[\"stem_index\"], n_stems\n",
        "        )\n",
        "        if seq.numel() > max_len:\n",
        "            seq = seq[:max_len]\n",
        "            mask = mask[:max_len]\n",
        "        seqs.append(seq)\n",
        "        masks.append(mask)\n",
        "        meta.append((item[\"track\"], item[\"stem\"]))\n",
        "\n",
        "    T = max(s.numel() for s in seqs)\n",
        "    T = min(T, max_len)\n",
        "    pad_id = spec.EOS  # pad with EOS\n",
        "\n",
        "    # Use the device of the generated sequences\n",
        "    device = seqs[0].device if seqs else torch.device('cpu')\n",
        "\n",
        "    x = torch.full((len(batch), T), pad_id, dtype=torch.int64, device=device)\n",
        "    m = torch.zeros((len(batch), T), dtype=torch.bool, device=device)\n",
        "\n",
        "    for i, (s, mk) in enumerate(zip(seqs, masks)):\n",
        "        t = min(s.numel(), T)\n",
        "        x[i, :t] = s[:t]\n",
        "        m[i, :t] = mk[:t]\n",
        "\n",
        "    return {\"idx\": x, \"loss_mask\": m, \"meta\": meta}"
      ],
      "metadata": {
        "id": "Bw-KngLYQJ3o"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Metrics: SI-SDR\n",
        "# -------------------------\n",
        "def si_sdr(est: torch.Tensor, ref: torch.Tensor, eps: float = 1e-8) -> float:\n",
        "    \"\"\"\n",
        "    Scale-Invariant SDR in dB for 1D tensors.\n",
        "    \"\"\"\n",
        "    est = est.detach().cpu().float()\n",
        "    ref = ref.detach().cpu().float()\n",
        "    ref_energy = torch.sum(ref * ref) + eps\n",
        "    alpha = torch.sum(est * ref) / ref_energy\n",
        "    s_target = alpha * ref\n",
        "    e_noise = est - s_target\n",
        "    ratio = (torch.sum(s_target * s_target) + eps) / (torch.sum(e_noise * e_noise) + eps)\n",
        "    return float(10.0 * torch.log10(ratio))\n"
      ],
      "metadata": {
        "id": "h3UJEeMsQYxP"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Training / validation\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    dataset_root: str\n",
        "    out_dir: str\n",
        "    sample_rate: int = 44100\n",
        "    window_sec: float = 0.1\n",
        "    n_fft: int = 256\n",
        "    hop: int = 256\n",
        "    n_mels: int = 64\n",
        "\n",
        "    d_model: int = 512\n",
        "    n_layers: int = 8\n",
        "    n_heads: int = 8\n",
        "    dropout: float = 0.1\n",
        "    max_seq_len: int = 4096\n",
        "\n",
        "    batch_size: int = 8\n",
        "    grad_accum: int = 8  # effective batch = batch_size * grad_accum\n",
        "    lr: float = 3e-5\n",
        "    min_lr: float = 3e-5\n",
        "    weight_decay: float = 0.05\n",
        "    betas: Tuple[float, float] = (0.9, 0.95)\n",
        "    eps: float = 1e-8\n",
        "    clip_norm: float = 1.0\n",
        "    warmup_steps: int = 1000\n",
        "    total_steps: int = 5000\n",
        "\n",
        "    val_every: int = 1000\n",
        "    ckpt_every: int = 2500\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    seed: int = 1337\n",
        "\n",
        "    mix_from_stems: bool = True\n",
        "    num_workers: int = 2\n",
        "\n",
        "    # evaluation\n",
        "    sisdr_eval_every: int = 5000\n",
        "    sisdr_patience_evals: int = 3\n",
        "    sisdr_min_delta_db: float = 0.1\n",
        "\n",
        "    # metrics output\n",
        "    metrics_dir: Optional[str] = None\n"
      ],
      "metadata": {
        "id": "k9MYDlI1QmMx"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_lr(step: int, cfg: TrainConfig) -> float:\n",
        "    if step < cfg.warmup_steps:\n",
        "        return cfg.lr * (step + 1) / max(1, cfg.warmup_steps)\n",
        "    t = (step - cfg.warmup_steps) / max(1, (cfg.total_steps - cfg.warmup_steps))\n",
        "    t = min(max(t, 0.0), 1.0)\n",
        "    # cosine from lr -> min_lr\n",
        "    return cfg.min_lr + 0.5 * (cfg.lr - cfg.min_lr) * (1.0 + math.cos(math.pi * t))\n",
        "\n",
        "\n",
        "def masked_ce_loss(logits: torch.Tensor, targets: torch.Tensor, loss_mask: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    logits: (B, T, V)\n",
        "    targets: (B, T)\n",
        "    loss_mask: (B, T) bool - only positions True contribute\n",
        "    \"\"\"\n",
        "    B, T, V = logits.shape\n",
        "    # shift: predict targets[:, 1:] from logits[:, :-1]\n",
        "    logits_s = logits[:, :-1, :].contiguous()\n",
        "    targets_s = targets[:, 1:].contiguous()\n",
        "    mask_s = loss_mask[:, 1:].contiguous()\n",
        "\n",
        "    # flatten\n",
        "    logits_f = logits_s.view(-1, V)\n",
        "    targets_f = targets_s.view(-1)\n",
        "    mask_f = mask_s.view(-1)\n",
        "\n",
        "    if mask_f.sum().item() == 0:\n",
        "        return torch.tensor(0.0, device=logits.device)\n",
        "\n",
        "    loss = F.cross_entropy(logits_f[mask_f], targets_f[mask_f])\n",
        "    return loss\n",
        "\n",
        "\n",
        "def save_ckpt(path: str, model: nn.Module, optim: torch.optim.Optimizer, step: int, cfg: TrainConfig, extra: dict):\n",
        "    payload = {\n",
        "        \"step\": step,\n",
        "        \"cfg\": cfg.__dict__,\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optim\": optim.state_dict(),\n",
        "        \"extra\": extra,\n",
        "    }\n",
        "    torch.save(payload, path)\n",
        "\n",
        "\n",
        "def load_ckpt(path: str, model: nn.Module, optim: Optional[torch.optim.Optimizer] = None):\n",
        "    ckpt = torch.load(path, map_location=\"cpu\")\n",
        "    model.load_state_dict(ckpt[\"model\"])\n",
        "    if optim is not None and \"optim\" in ckpt:\n",
        "        optim.load_state_dict(ckpt[\"optim\"])\n",
        "    return ckpt\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_model(model: nn.Module, loader, spec: TokenSpec, device: str) -> float:\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    for batch in loader:\n",
        "        idx = batch[\"idx\"].to(device)\n",
        "        m = batch[\"loss_mask\"].to(device)\n",
        "        logits = model(idx)\n",
        "        loss = masked_ce_loss(logits, idx, m)\n",
        "        losses.append(float(loss.detach().cpu()))\n",
        "    model.train()\n",
        "    return float(np.mean(losses)) if losses else float(\"nan\")\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_decode_target_tokens(\n",
        "    model: nn.Module,\n",
        "    prompt: torch.Tensor,      # (T0,)\n",
        "    n_decode: int,\n",
        "    device: str,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Greedy decode n_decode tokens following prompt. Returns full sequence (prompt + decoded).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    seq = prompt.clone().to(device).unsqueeze(0)  # (1, T)\n",
        "    for _ in range(n_decode):\n",
        "        logits = model(seq)  # (1, T, V)\n",
        "        next_tok = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)  # (1,1)\n",
        "        seq = torch.cat([seq, next_tok], dim=1)\n",
        "        if seq.shape[1] >= model.max_seq_len:\n",
        "            break\n",
        "    model.train()\n",
        "    return seq.squeeze(0).detach().cpu()\n"
      ],
      "metadata": {
        "id": "hfMmTpZ5QzaY"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Inference: overlap-add\n",
        "# -------------------------\n",
        "def hann_fade(win_len: int, device: str) -> torch.Tensor:\n",
        "    # Half-overlap Hann-ish fade for OLA smoothing\n",
        "    w = torch.hann_window(win_len, periodic=False, device=device)\n",
        "    return w\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def separate_window(\n",
        "    model: GPTSeparator,\n",
        "    tokenizer: AudioTokenizer,\n",
        "    spec: TokenSpec,\n",
        "    mix_wav: torch.Tensor,\n",
        "    stem_index: int,\n",
        "    n_stems: int,\n",
        "    device: str,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Separate one window of waveform using mixture phase.\n",
        "    \"\"\"\n",
        "    mix_tok2d, phase = tokenizer.wav_to_tokens(mix_wav)\n",
        "    mix_flat = mix_tok2d.reshape(-1)\n",
        "\n",
        "    # Build prompt up to 'target' token (inclusive)\n",
        "    stem_tok = spec.stem_token(stem_index)\n",
        "    prompt = torch.cat([\n",
        "        torch.tensor([spec.BOS, spec.MIX, spec.mix], dtype=torch.int64),\n",
        "        mix_flat.to(torch.int64),\n",
        "        torch.tensor([spec.SEP, spec.STEM, stem_tok, spec.target], dtype=torch.int64),\n",
        "    ], dim=0)\n",
        "\n",
        "    # Decode exactly the number of target tokens needed\n",
        "    n_tgt = mix_flat.numel()  # same shape as mix in baseline\n",
        "    full = greedy_decode_target_tokens(model, prompt, n_tgt + 1, device=device)  # +EOS token maybe\n",
        "    # Extract decoded target tokens\n",
        "    decoded = full[prompt.numel():prompt.numel() + n_tgt]\n",
        "    decoded2d = decoded.view(mix_tok2d.shape[0], mix_tok2d.shape[1]).clamp(0, spec.n_mag_tokens - 1)\n",
        "\n",
        "    # Reconstruct waveform using mixture phase\n",
        "    wav_est = tokenizer.tokens_to_wav_with_phase(decoded2d, phase, length=mix_wav.numel())\n",
        "    return wav_est\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def overlap_add_separate(\n",
        "    model: GPTSeparator,\n",
        "    tokenizer: AudioTokenizer,\n",
        "    spec: TokenSpec,\n",
        "    mix_wav: torch.Tensor,\n",
        "    stem_index: int,\n",
        "    n_stems: int,\n",
        "    window_sec: float,\n",
        "    hop_sec: float,\n",
        "    device: str,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Full-file separation by OLA with Hann fade.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    sr = tokenizer.sr\n",
        "    win_len = int(round(window_sec * sr))\n",
        "    hop_len = int(round(hop_sec * sr))\n",
        "    if hop_len <= 0 or win_len <= 0:\n",
        "        raise ValueError(\"Invalid window/hop for overlap-add.\")\n",
        "    if mix_wav.numel() < win_len:\n",
        "        # pad\n",
        "        pad = win_len - mix_wav.numel()\n",
        "        mix_wav = torch.cat([mix_wav, torch.zeros(pad)], dim=0)\n",
        "\n",
        "    w = hann_fade(win_len, device=device).detach().cpu()\n",
        "    out = torch.zeros_like(mix_wav)\n",
        "    norm = torch.zeros_like(mix_wav)\n",
        "\n",
        "    for start in range(0, mix_wav.numel() - win_len + 1, hop_len):\n",
        "        chunk = mix_wav[start:start + win_len].to(device)\n",
        "        est = separate_window(model, tokenizer, spec, chunk, stem_index, n_stems, device=device).detach().cpu()\n",
        "        out[start:start + win_len] += est * w\n",
        "        norm[start:start + win_len] += w\n",
        "\n",
        "    out = out / norm.clamp_min(1e-6)\n",
        "    model.train()\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "kPTNOvrFP6Aw"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Evaluation\n",
        "# -------------------------\n",
        "@torch.no_grad()\n",
        "def evaluate_sisdr_windows(\n",
        "    model: GPTSeparator,\n",
        "    dataset: StemSeparationDataset,\n",
        "    tokenizer: AudioTokenizer,\n",
        "    spec: TokenSpec,\n",
        "    stems: List[str],\n",
        "    n_windows: int,\n",
        "    out_csv: str,\n",
        "    device: str,\n",
        "):\n",
        "    \"\"\"\n",
        "    Window-level evaluation: greedy decode each sampled window and compute SI-SDR and CE on masked tokens.\n",
        "    \"\"\"\n",
        "    ensure_dir(os.path.dirname(out_csv) or \".\")\n",
        "    model.eval()\n",
        "\n",
        "    rows = []\n",
        "    for i in range(n_windows):\n",
        "        item = dataset[i]  # dataset randomized anyway\n",
        "        mix = item[\"mixture_wav\"].to(device)\n",
        "        tgt = item[\"target_wav\"].to(device)\n",
        "\n",
        "        mix_tok2d, phase = tokenizer.wav_to_tokens(mix)\n",
        "        tgt_tok2d, _ = tokenizer.wav_to_tokens(tgt)\n",
        "\n",
        "        seq, mask = build_sequence_tokens(spec, mix_tok2d, tgt_tok2d, item[\"stem_index\"], len(stems))\n",
        "        idx = seq.unsqueeze(0).to(device)\n",
        "        m = mask.unsqueeze(0).to(device)\n",
        "\n",
        "        logits = model(idx)\n",
        "        ce = float(masked_ce_loss(logits, idx, m).detach().cpu())\n",
        "\n",
        "        # decode target tokens and reconstruct\n",
        "        est = separate_window(model, tokenizer, spec, mix, item[\"stem_index\"], len(stems), device=device)\n",
        "        s = si_sdr(est, tgt)\n",
        "\n",
        "        rows.append({\n",
        "            \"track\": item[\"track\"],\n",
        "            \"start_sample\": -1,  # unknown in this virtual sampler baseline\n",
        "            \"stem\": item[\"stem\"],\n",
        "            \"sisdr_db\": s,\n",
        "            \"ce_loss\": ce,\n",
        "            \"num_masked_tokens\": int(mask.sum().item()),\n",
        "        })\n",
        "\n",
        "    # write\n",
        "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        wcsv = csv.DictWriter(f, fieldnames=list(rows[0].keys()) if rows else [])\n",
        "        wcsv.writeheader()\n",
        "        for r in rows:\n",
        "            wcsv.writerow(r)\n",
        "\n",
        "    model.train()\n",
        "    return rows\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_sisdr_tracks(\n",
        "    model: GPTSeparator,\n",
        "    dataset_root: str,\n",
        "    tokenizer: AudioTokenizer,\n",
        "    spec: TokenSpec,\n",
        "    stems: List[str],\n",
        "    out_dir: str,\n",
        "    device: str,\n",
        "    window_sec: float,\n",
        "    hop_sec: float,\n",
        "    max_tracks: Optional[int] = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Track-level evaluation: full-file separation per stem, SI-SDR per stem and per track.\n",
        "    \"\"\"\n",
        "    ensure_dir(out_dir)\n",
        "    tracks = list_tracks(dataset_root)\n",
        "    if max_tracks is not None:\n",
        "        tracks = tracks[:max_tracks]\n",
        "\n",
        "    per_stem_rows = []\n",
        "    per_track_rows = []\n",
        "\n",
        "    for td in tracks:\n",
        "        track_name = os.path.basename(td)\n",
        "        mix_path = os.path.join(td, \"mixture.wav\")\n",
        "        if not os.path.exists(mix_path):\n",
        "            continue\n",
        "        mix = load_wav_mono(mix_path, tokenizer.sr)\n",
        "\n",
        "        stem_scores = []\n",
        "        for si, stem in enumerate(stems):\n",
        "            stem_path = os.path.join(td, stem)\n",
        "            if not os.path.exists(stem_path):\n",
        "                continue\n",
        "            ref = load_wav_mono(stem_path, tokenizer.sr)\n",
        "\n",
        "            est = overlap_add_separate(\n",
        "                model, tokenizer, spec, mix, si, len(stems),\n",
        "                window_sec=window_sec, hop_sec=hop_sec, device=device\n",
        "            )\n",
        "\n",
        "            L = min(est.numel(), ref.numel())\n",
        "            s = si_sdr(est[:L], ref[:L])\n",
        "            stem_scores.append(s)\n",
        "\n",
        "            per_stem_rows.append({\n",
        "                \"track\": track_name,\n",
        "                \"stem\": stem,\n",
        "                \"sisdr_db\": s,\n",
        "            })\n",
        "\n",
        "        if stem_scores:\n",
        "            per_track_rows.append({\n",
        "                \"track\": track_name,\n",
        "                \"sisdr_db_mean\": float(np.mean(stem_scores)),\n",
        "                \"sisdr_db_min\": float(np.min(stem_scores)),\n",
        "                \"sisdr_db_max\": float(np.max(stem_scores)),\n",
        "            })\n",
        "\n",
        "    # export CSVs\n",
        "    per_stem_csv = os.path.join(out_dir, \"sisdr_per_stem.csv\")\n",
        "    per_track_csv = os.path.join(out_dir, \"sisdr_per_track.csv\")\n",
        "\n",
        "    if per_stem_rows:\n",
        "        with open(per_stem_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            wcsv = csv.DictWriter(f, fieldnames=list(per_stem_rows[0].keys()))\n",
        "            wcsv.writeheader()\n",
        "            wcsv.writerows(per_stem_rows)\n",
        "\n",
        "    if per_track_rows:\n",
        "        with open(per_track_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            wcsv = csv.DictWriter(f, fieldnames=list(per_track_rows[0].keys()))\n",
        "            wcsv.writeheader()\n",
        "            wcsv.writerows(per_track_rows)\n",
        "\n",
        "    return per_stem_csv, per_track_csv\n"
      ],
      "metadata": {
        "id": "FuPAIGgWPx5Y"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Train loop\n",
        "# -------------------------\n",
        "def split_dataset(tracks: List[str], val_frac: float = 0.1, seed: int = 1337) -> Tuple[List[str], List[str]]:\n",
        "    rng = random.Random(seed)\n",
        "    tracks = tracks[:]\n",
        "    rng.shuffle(tracks)\n",
        "    n_val = max(1, int(round(len(tracks) * val_frac)))\n",
        "    return tracks[n_val:], tracks[:n_val]\n",
        "\n",
        "def _global_collate_fn(batch: List[dict], tokenizer: 'AudioTokenizer', spec: TokenSpec, n_stems: int, max_len: int):\n",
        "    return collate_batch(batch, tokenizer=tokenizer, spec=spec, n_stems=n_stems, max_len=max_len)\n",
        "\n",
        "def train_model(cfg: TrainConfig):\n",
        "    set_seed(cfg.seed)\n",
        "    ensure_dir(cfg.out_dir)\n",
        "    ckpt_dir = os.path.join(cfg.out_dir, \"checkpoints\")\n",
        "    ensure_dir(ckpt_dir)\n",
        "    metrics_dir = cfg.metrics_dir or os.path.join(cfg.out_dir, \"metrics\")\n",
        "    ensure_dir(metrics_dir)\n",
        "\n",
        "    # Dataset split by track\n",
        "    all_tracks = list_tracks(cfg.dataset_root)\n",
        "    train_tracks, val_tracks = split_dataset(all_tracks, val_frac=0.1, seed=cfg.seed)\n",
        "\n",
        "    train_ds = StemSeparationDataset(\n",
        "        cfg.dataset_root, sample_rate=cfg.sample_rate, window_sec=cfg.window_sec,\n",
        "        mix_from_stems=cfg.mix_from_stems, explicit_tracks=train_tracks, seed=cfg.seed\n",
        "    )\n",
        "    val_ds = StemSeparationDataset(\n",
        "        cfg.dataset_root, sample_rate=cfg.sample_rate, window_sec=cfg.window_sec,\n",
        "        mix_from_stems=cfg.mix_from_stems, explicit_tracks=val_tracks, seed=cfg.seed + 1\n",
        "    )\n",
        "\n",
        "    stems = train_ds.stem_names\n",
        "    n_stems = len(stems)\n",
        "    spec = TokenSpec()\n",
        "\n",
        "    device = cfg.device\n",
        "    tokenizer = AudioTokenizer(\n",
        "        sample_rate=cfg.sample_rate, n_fft=cfg.n_fft, hop=cfg.hop, n_mels=cfg.n_mels,\n",
        "        token_spec=spec, device=device\n",
        "    )\n",
        "\n",
        "    vocab_total = spec.total_vocab(n_stems)\n",
        "\n",
        "    model = GPTSeparator(\n",
        "        vocab_size=vocab_total,\n",
        "        d_model=cfg.d_model,\n",
        "        n_layers=cfg.n_layers,\n",
        "        n_heads=cfg.n_heads,\n",
        "        dropout=cfg.dropout,\n",
        "        max_seq_len=cfg.max_seq_len,\n",
        "    ).to(device)\n",
        "\n",
        "    optim = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=cfg.lr,\n",
        "        betas=cfg.betas,\n",
        "        eps=cfg.eps,\n",
        "        weight_decay=cfg.weight_decay,\n",
        "    )\n",
        "\n",
        "    # Load latest checkpoint if exists\n",
        "    latest_path = os.path.join(ckpt_dir, \"ckpt_latest.pt\")\n",
        "    start_step = 0\n",
        "    best_sisdr = -1e9\n",
        "    best_step = -1\n",
        "    sisdr_no_improve = 0\n",
        "    if os.path.exists(latest_path):\n",
        "        ck = load_ckpt(latest_path, model, optim)\n",
        "        start_step = int(ck.get(\"step\", 0))\n",
        "        extra = ck.get(\"extra\", {})\n",
        "        best_sisdr = float(extra.get(\"best_sisdr\", best_sisdr))\n",
        "        best_step = int(extra.get(\"best_step\", best_step))\n",
        "        sisdr_no_improve = int(extra.get(\"sisdr_no_improve\", 0))\n",
        "        print(f\"Resumed from {latest_path} at step={start_step}\")\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_ds, batch_size=cfg.batch_size, shuffle=False,  # dataset samples randomly\n",
        "        num_workers=cfg.num_workers,\n",
        "        collate_fn=functools.partial(_global_collate_fn, tokenizer=tokenizer, spec=spec, n_stems=n_stems, max_len=cfg.max_seq_len),\n",
        "        pin_memory=False\n",
        "    )\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_ds, batch_size=cfg.batch_size, shuffle=False,\n",
        "        num_workers=max(0, cfg.num_workers // 2),\n",
        "        collate_fn=functools.partial(_global_collate_fn, tokenizer=tokenizer, spec=spec, n_stems=n_stems, max_len=cfg.max_seq_len),\n",
        "        pin_memory=False\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    t0 = time.time()\n",
        "    running = 0.0\n",
        "    step = start_step\n",
        "\n",
        "    # simple iterator cycling\n",
        "    train_iter = iter(train_loader)\n",
        "\n",
        "    # Save config snapshot\n",
        "    with open(os.path.join(cfg.out_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(cfg.__dict__, f, indent=2)\n",
        "    with open(os.path.join(cfg.out_dir, \"stems.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(stems, f, indent=2)\n",
        "\n",
        "    print(f\"Device={device} | stems={stems} | effective_batch={cfg.batch_size * cfg.grad_accum}\")\n",
        "\n",
        "    while step < cfg.total_steps:\n",
        "        # LR schedule\n",
        "        lr_now = cosine_lr(step, cfg)\n",
        "        for pg in optim.param_groups:\n",
        "            pg[\"lr\"] = lr_now\n",
        "\n",
        "        # fetch batch\n",
        "        try:\n",
        "            batch = next(train_iter)\n",
        "        except StopIteration:\n",
        "            train_iter = iter(train_loader)\n",
        "            batch = next(train_iter)\n",
        "\n",
        "        idx = batch[\"idx\"].to(device, non_blocking=True)\n",
        "        m = batch[\"loss_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "        logits = model(idx)\n",
        "        loss = masked_ce_loss(logits, idx, m) / cfg.grad_accum\n",
        "        loss.backward()\n",
        "\n",
        "        if (step + 1) % cfg.grad_accum == 0:\n",
        "            if cfg.clip_norm is not None and cfg.clip_norm > 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_norm)\n",
        "            optim.step()\n",
        "            optim.zero_grad(set_to_none=True)\n",
        "\n",
        "        running += float(loss.detach().cpu()) * cfg.grad_accum  # unscale for reporting\n",
        "\n",
        "        # validate CE\n",
        "        if (step + 1) % cfg.val_every == 0:\n",
        "            val_ce = validate_model(model, val_loader, spec, device=device)\n",
        "            avg = running / cfg.val_every\n",
        "            running = 0.0\n",
        "            elapsed = time.time() - t0\n",
        "            print(\n",
        "                f\"step {step+1}/{cfg.total_steps} | lr {lr_now:.2e} | \"\n",
        "                f\"train_ce {avg:.4f} | val_ce {val_ce:.4f} | time {human_time(elapsed)}\"\n",
        "            )\n",
        "\n",
        "        # SI-SDR eval (window-level quick proxy)\n",
        "        if (step + 1) % cfg.sisdr_eval_every == 0:\n",
        "            out_csv = os.path.join(metrics_dir, \"sisdr_windows.csv\")\n",
        "            rows = evaluate_sisdr_windows(\n",
        "                model, val_ds, tokenizer, spec, stems,\n",
        "                n_windows=64, out_csv=out_csv, device=device\n",
        "            )\n",
        "            mean_s = float(np.mean([r[\"sisdr_db\"] for r in rows])) if rows else float(\"-inf\")\n",
        "            print(f\"  SI-SDR(window mean over 64): {mean_s:.3f} dB  (csv: {out_csv})\")\n",
        "\n",
        "            # early stopping on SI-SDR improvements\n",
        "            if mean_s >= best_sisdr + cfg.sisdr_min_delta_db:\n",
        "                best_sisdr = mean_s\n",
        "                best_step = step + 1\n",
        "                sisdr_no_improve = 0\n",
        "                save_ckpt(\n",
        "                    os.path.join(ckpt_dir, \"ckpt_best.pt\"),\n",
        "                    model, optim, step + 1, cfg,\n",
        "                    extra={\"best_sisdr\": best_sisdr, \"best_step\": best_step, \"sisdr_no_improve\": sisdr_no_improve}\n",
        "                )\n",
        "                print(f\"   New best SI-SDR: {best_sisdr:.3f} dB at step {best_step}\")\n",
        "            else:\n",
        "                sisdr_no_improve += 1\n",
        "                print(f\"  No SI-SDR improvement (patience {sisdr_no_improve}/{cfg.sisdr_patience_evals})\")\n",
        "                if sisdr_no_improve >= cfg.sisdr_patience_evals:\n",
        "                    print(f\" Early stopping: SI-SDR plateaued. Best={best_sisdr:.3f} dB at step {best_step}\")\n",
        "                    break\n",
        "\n",
        "        # checkpoints\n",
        "        if (step + 1) % cfg.ckpt_every == 0:\n",
        "            save_ckpt(\n",
        "                latest_path, model, optim, step + 1, cfg,\n",
        "                extra={\"best_sisdr\": best_sisdr, \"best_step\": best_step, \"sisdr_no_improve\": sisdr_no_improve}\n",
        "            )\n",
        "            print(f\"  Saved checkpoint: {latest_path}\")\n",
        "\n",
        "        step += 1\n",
        "\n",
        "    # final save\n",
        "    save_ckpt(\n",
        "        latest_path, model, optim, step, cfg,\n",
        "        extra={\"best_sisdr\": best_sisdr, \"best_step\": best_step, \"sisdr_no_improve\": sisdr_no_improve}\n",
        "    )\n",
        "    print(f\"Done. Final step={step}. Best SI-SDR(window mean)={best_sisdr:.3f} dB at step={best_step}.\")"
      ],
      "metadata": {
        "id": "U7_yV9ymPiqQ"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Notebook-friendly helpers\n",
        "# -------------------------\n",
        "def init_model(ckpt_path: str, device: Optional[str] = None):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    ck = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    cfgd = ck.get(\"cfg\", {})\n",
        "    stems_path = os.path.join(os.path.dirname(os.path.dirname(ckpt_path)), \"stems.json\")\n",
        "    stems = json.load(open(stems_path, \"r\", encoding=\"utf-8\")) if os.path.exists(stems_path) else []\n",
        "\n",
        "    cfg = TrainConfig(dataset_root=cfgd.get(\"dataset_root\", \"\"), out_dir=cfgd.get(\"out_dir\", \"\"),\n",
        "                      sample_rate=cfgd.get(\"sample_rate\", 44100),\n",
        "                      window_sec=cfgd.get(\"window_sec\", 0.1),\n",
        "                      n_fft=cfgd.get(\"n_fft\", 256), hop=cfgd.get(\"hop\", 256),\n",
        "                      n_mels=cfgd.get(\"n_mels\", 64),\n",
        "                      d_model=cfgd.get(\"d_model\", 512), n_layers=cfgd.get(\"n_layers\", 8), n_heads=cfgd.get(\"n_heads\", 8),\n",
        "                      dropout=cfgd.get(\"dropout\", 0.1), max_seq_len=cfgd.get(\"max_seq_len\", 4096),\n",
        "                      device=device)\n",
        "\n",
        "    spec = TokenSpec()\n",
        "    vocab_total = spec.total_vocab(len(stems) if stems else 4)  # fallback\n",
        "    model = GPTSeparator(vocab_size=vocab_total, d_model=cfg.d_model, n_layers=cfg.n_layers,\n",
        "                         n_heads=cfg.n_heads, dropout=cfg.dropout, max_seq_len=cfg.max_seq_len).to(device)\n",
        "\n",
        "    model.load_state_dict(ck[\"model\"])\n",
        "    model.eval()\n",
        "\n",
        "    tokenizer = AudioTokenizer(sample_rate=cfg.sample_rate, n_fft=cfg.n_fft, hop=cfg.hop, n_mels=cfg.n_mels,\n",
        "                               token_spec=spec, device=device)\n",
        "\n",
        "    return model, tokenizer, spec, stems, cfg\n",
        "\n",
        "\n",
        "def run_separation(model, tokenizer, spec, stems: List[str], mixture_wav_path: str, out_wav_path: str,\n",
        "                   stem_name: str = \"vocals\", window_sec: float = 0.1, hop_sec: float = 0.05, device: Optional[str] = None):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    mix = load_wav_mono(mixture_wav_path, tokenizer.sr)\n",
        "    if stem_name not in stems:\n",
        "        raise ValueError(f\"stem_name={stem_name} not in stems={stems}\")\n",
        "    si = stems.index(stem_name)\n",
        "\n",
        "    est = overlap_add_separate(model, tokenizer, spec, mix, si, len(stems), window_sec, hop_sec, device=device)\n",
        "    est = est.unsqueeze(0)  # (1, T)\n",
        "    torchaudio.save(out_wav_path, est.cpu(), tokenizer.sr)\n",
        "    return out_wav_path\n",
        "\n",
        "\n",
        "def run_demo():\n",
        "    print(\"Demo placeholder: load a checkpoint with init_model(), then run_separation().\")\n"
      ],
      "metadata": {
        "id": "H_hqPaT0PU8_"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_args(argv=None):\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--dataset_root\", type=str, required=True)\n",
        "    p.add_argument(\"--out_dir\", type=str, default=\"./runs\")\n",
        "    p.add_argument(\"--ckpt\", type=str, default=\"\")\n",
        "    p.add_argument(\"--device\", type=str, default=\"\")\n",
        "\n",
        "    p.add_argument(\"--sample_rate\", type=int, default=44100)\n",
        "    p.add_argument(\"--window_sec\", type=float, default=0.1)\n",
        "    p.add_argument(\"--n_fft\", type=int, default=256)\n",
        "    p.add_argument(\"--hop\", type=int, default=256)\n",
        "    p.add_argument(\"--n_mels\", type=int, default=64)\n",
        "\n",
        "    p.add_argument(\"--d_model\", type=int, default=512)\n",
        "    p.add_argument(\"--n_layers\", type=int, default=8)\n",
        "    p.add_argument(\"--n_heads\", type=int, default=8)\n",
        "    p.add_argument(\"--dropout\", type=float, default=0.1)\n",
        "    p.add_argument(\"--max_seq_len\", type=int, default=4096)\n",
        "\n",
        "    p.add_argument(\"--batch_size\", type=int, default=8)\n",
        "    p.add_argument(\"--grad_accum\", type=int, default=8)\n",
        "    p.add_argument(\"--lr\", type=float, default=3e-4)\n",
        "    p.add_argument(\"--min_lr\", type=float, default=3e-5)\n",
        "    p.add_argument(\"--weight_decay\", type=float, default=0.05)\n",
        "    p.add_argument(\"--warmup_steps\", type=int, default=2000)\n",
        "    p.add_argument(\"--total_steps\", type=int, default=150_000)\n",
        "    p.add_argument(\"--val_every\", type=int, default=1000)\n",
        "    p.add_argument(\"--ckpt_every\", type=int, default=5000)\n",
        "    p.add_argument(\"--sisdr_eval_every\", type=int, default=5000)\n",
        "    p.add_argument(\"--mix_from_stems\", action=\"store_true\")\n",
        "\n",
        "    p.add_argument(\"--metrics_dir\", type=str, default=\"\")\n",
        "    p.add_argument(\"--seed\", type=int, default=1337)\n",
        "    p.add_argument(\"--num_workers\", type=int, default=2)\n",
        "\n",
        "    p.add_argument(\"cmd\", choices=[\"train\", \"eval_windows\", \"eval_tracks\"])\n",
        "    p.add_argument(\"--eval_out\", type=str, default=\"\")\n",
        "    p.add_argument(\"--eval_tracks_max\", type=int, default=0)\n",
        "    p.add_argument(\"--ola_hop_sec\", type=float, default=0.05)\n",
        "\n",
        "    return p.parse_args(argv)"
      ],
      "metadata": {
        "id": "gvcHijFxO-7_"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(argv=None):\n",
        "    args = parse_args(argv)\n",
        "\n",
        "    device = args.device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    cfg = TrainConfig(\n",
        "        dataset_root=args.dataset_root,\n",
        "        out_dir=args.out_dir,\n",
        "        sample_rate=args.sample_rate,\n",
        "        window_sec=args.window_sec,\n",
        "        n_fft=args.n_fft,\n",
        "        hop=args.hop,\n",
        "        n_mels=args.n_mels,\n",
        "        d_model=args.d_model,\n",
        "        n_layers=args.n_layers,\n",
        "        n_heads=args.n_heads,\n",
        "        dropout=args.dropout,\n",
        "        max_seq_len=args.max_seq_len,\n",
        "        batch_size=args.batch_size,\n",
        "        grad_accum=args.grad_accum,\n",
        "        lr=args.lr,\n",
        "        min_lr=args.min_lr,\n",
        "        weight_decay=args.weight_decay,\n",
        "        warmup_steps=args.warmup_steps,\n",
        "        total_steps=args.total_steps,\n",
        "        val_every=args.val_every,\n",
        "        ckpt_every=args.ckpt_every,\n",
        "        sisdr_eval_every=args.sisdr_eval_every,\n",
        "        mix_from_stems=args.mix_from_stems,\n",
        "        metrics_dir=args.metrics_dir if args.metrics_dir else None,\n",
        "        seed=args.seed,\n",
        "        num_workers=args.num_workers,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    if args.cmd == \"train\":\n",
        "        train_model(cfg)\n",
        "        return\n",
        "\n",
        "    # For eval, load checkpoint\n",
        "    spec = TokenSpec()\n",
        "    stems_path = os.path.join(cfg.out_dir, \"stems.json\")\n",
        "    if not os.path.exists(stems_path):\n",
        "        # fallback: infer stems from dataset\n",
        "        ds_tmp = StemSeparationDataset(cfg.dataset_root, sample_rate=cfg.sample_rate, window_sec=cfg.window_sec,\n",
        "                                       mix_from_stems=cfg.mix_from_stems, seed=cfg.seed)\n",
        "        stems = ds_tmp.stem_names\n",
        "    else:\n",
        "        stems = json.load(open(stems_path, \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "    n_stems = len(stems)\n",
        "    tokenizer = AudioTokenizer(cfg.sample_rate, cfg.n_fft, cfg.hop, cfg.n_mels, token_spec=spec, device=device)\n",
        "    model = GPTSeparator(\n",
        "        vocab_size=spec.total_vocab(n_stems),\n",
        "        d_model=cfg.d_model, n_layers=cfg.n_layers, n_heads=cfg.n_heads,\n",
        "        dropout=cfg.dropout, max_seq_len=cfg.max_seq_len\n",
        "    ).to(device)\n",
        "\n",
        "    ckpt_path = args.ckpt if args.ckpt else os.path.join(cfg.out_dir, \"checkpoints\", \"ckpt_latest.pt\")\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
        "    load_ckpt(ckpt_path, model, None)\n",
        "    model.eval()\n",
        "\n",
        "    if args.cmd == \"eval_windows\":\n",
        "        ds = StemSeparationDataset(cfg.dataset_root, sample_rate=cfg.sample_rate, window_sec=cfg.window_sec,\n",
        "                                   mix_from_stems=cfg.mix_from_stems, seed=cfg.seed + 1)\n",
        "        out_csv = args.eval_out or os.path.join(cfg.metrics_dir or os.path.join(cfg.out_dir, \"metrics\"), \"sisdr_windows.csv\")\n",
        "        evaluate_sisdr_windows(model, ds, tokenizer, spec, stems, n_windows=256, out_csv=out_csv, device=device)\n",
        "        print(f\"Wrote: {out_csv}\")\n",
        "        return\n",
        "\n",
        "    if args.cmd == \"eval_tracks\":\n",
        "        out_dir = args.eval_out or (cfg.metrics_dir or os.path.join(cfg.out_dir, \"metrics\"))\n",
        "        max_tracks = args.eval_tracks_max if args.eval_tracks_max > 0 else None\n",
        "        per_stem_csv, per_track_csv = evaluate_sisdr_tracks(\n",
        "            model, cfg.dataset_root, tokenizer, spec, stems,\n",
        "            out_dir=out_dir, device=device,\n",
        "            window_sec=cfg.window_sec, hop_sec=args.ola_hop_sec, max_tracks=max_tracks\n",
        "        )\n",
        "        print(f\"Wrote: {per_stem_csv}\")\n",
        "        print(f\"Wrote: {per_track_csv}\")\n",
        "        return"
      ],
      "metadata": {
        "id": "lE7iwRWvO02_"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.multiprocessing\n",
        "torch.multiprocessing.set_start_method('spawn', force=True)\n",
        "\n",
        "# Retry training after fixing pin_memory\n",
        "main(argv=[\"--dataset_root\", \"/content/dataset_root\", \"--out_dir\", \"./runs/exp_notebook_train\", \"train\", \"--window_sec\", \"0.01\", \"--n_mels\", \"16\", \"--num_workers\", \"2\", \"--batch_size\", \"64\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "bwC7wrigMN49",
        "outputId": "1f37d0df-ba81-4d8a-91ca-11dac95af9b3"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device=cuda | stems=['bass.wav', 'drums.wav', 'other.wav', 'vocals.wav'] | effective_batch=64\n",
            "step 1000/150000 | lr 1.50e-04 | train_ce 1.3966 | val_ce 0.9076 | time 4.2h\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2399766252.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Retry training after fixing pin_memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"--dataset_root\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/dataset_root\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--out_dir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./runs/exp_notebook_train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--window_sec\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"0.01\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--n_mels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"16\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"--num_workers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-410214393.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3210904065.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# validate CE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mval_ce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_every\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mrunning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1122287441.py\u001b[0m in \u001b[0;36mvalidate_model\u001b[0;34m(model, loader, spec, device)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"idx\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-484512420.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# load target stem window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_wav_mono\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# mixture window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-952890978.py\u001b[0m in \u001b[0;36mload_wav_mono\u001b[0;34m(path, target_sr)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_wav_mono\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (C, T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mby\u001b[0m \u001b[0mTorchCodec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m     return load_with_torchcodec(\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mframe_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/_torchcodec.py\u001b[0m in \u001b[0;36mload_with_torchcodec\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# This is the simplest approach since torchcodec uses time-based indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0maudio_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to decode audio samples: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchcodec/decoders/_audio_decoder.py\u001b[0m in \u001b[0;36mget_all_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mAudioSamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0msamples\u001b[0m \u001b[0mwithin\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \"\"\"\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_samples_played_in_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     def get_samples_played_in_range(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchcodec/decoders/_audio_decoder.py\u001b[0m in \u001b[0;36mget_samples_played_in_range\u001b[0;34m(self, start_seconds, stop_seconds)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;34mf\"Invalid start seconds: {start_seconds}. It must be less than or equal to stop seconds ({stop_seconds}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             )\n\u001b[0;32m--> 137\u001b[0;31m         frames, first_pts = core.get_frames_by_pts_in_range_audio(\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mstart_seconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_seconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0;31m# Use positional-only argument to avoid naming collision with aten ops arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}