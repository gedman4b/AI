{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rawOWg0UelCl"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://docs.pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgXfB8AjelCm"
      },
      "source": [
        "[Learn the Basics](intro.html) \\|\\| **Quickstart** \\|\\|\n",
        "[Tensors](tensorqs_tutorial.html) \\|\\| [Datasets &\n",
        "DataLoaders](data_tutorial.html) \\|\\|\n",
        "[Transforms](transforms_tutorial.html) \\|\\| [Build\n",
        "Model](buildmodel_tutorial.html) \\|\\|\n",
        "[Autograd](autogradqs_tutorial.html) \\|\\|\n",
        "[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n",
        "Model](saveloadrun_tutorial.html)\n",
        "\n",
        "Quickstart\n",
        "==========\n",
        "\n",
        "This section runs through the API for common tasks in machine learning.\n",
        "Refer to the links in each section to dive deeper.\n",
        "\n",
        "Working with data\n",
        "-----------------\n",
        "\n",
        "PyTorch has two [primitives to work with\n",
        "data](https://pytorch.org/docs/stable/data.html):\n",
        "`torch.utils.data.DataLoader` and `torch.utils.data.Dataset`. `Dataset`\n",
        "stores the samples and their corresponding labels, and `DataLoader`\n",
        "wraps an iterable around the `Dataset`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "La6jsbzHelCn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpn8nnegelCn"
      },
      "source": [
        "PyTorch offers domain-specific libraries such as\n",
        "[TorchText](https://pytorch.org/text/stable/index.html),\n",
        "[TorchVision](https://pytorch.org/vision/stable/index.html), and\n",
        "[TorchAudio](https://pytorch.org/audio/stable/index.html), all of which\n",
        "include datasets. For this tutorial, we will be using a TorchVision\n",
        "dataset.\n",
        "\n",
        "The `torchvision.datasets` module contains `Dataset` objects for many\n",
        "real-world vision data like CIFAR, COCO ([full list\n",
        "here](https://pytorch.org/vision/stable/datasets.html)). In this\n",
        "tutorial, we use the FashionMNIST dataset. Every TorchVision `Dataset`\n",
        "includes two arguments: `transform` and `target_transform` to modify the\n",
        "samples and labels respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LKCYwi23elCn",
        "outputId": "db6ebbff-cfb4-4509-cf54-25643452f98d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 16.9MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 273kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 4.97MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 10.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykBt7owEelCn"
      },
      "source": [
        "We pass the `Dataset` as an argument to `DataLoader`. This wraps an\n",
        "iterable over our dataset, and supports automatic batching, sampling,\n",
        "shuffling and multiprocess data loading. Here we define a batch size of\n",
        "64, i.e. each element in the dataloader iterable will return a batch of\n",
        "64 features and labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g_0zxmqMelCo",
        "outputId": "cb067557-64a3-409f-fdf7-16fcd501006d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvXbtYDwelCo"
      },
      "source": [
        "Read more about [loading data in PyTorch](data_tutorial.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBDqnawOelCo"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPitgJIselCo"
      },
      "source": [
        "Creating Models\n",
        "===============\n",
        "\n",
        "To define a neural network in PyTorch, we create a class that inherits\n",
        "from\n",
        "[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
        "We define the layers of the network in the `__init__` function and\n",
        "specify how data will pass through the network in the `forward`\n",
        "function. To accelerate operations in the neural network, we move it to\n",
        "the\n",
        "[accelerator](https://pytorch.org/docs/stable/torch.html#accelerators)\n",
        "such as CUDA, MPS, MTIA, or XPU. If the current accelerator is\n",
        "available, we will use it. Otherwise, we use the CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "30aCqernelCo",
        "outputId": "392125c7-d931-4345-fb69-17c06e6d7d6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlmLyBhXelCp"
      },
      "source": [
        "Read more about [building neural networks in\n",
        "PyTorch](buildmodel_tutorial.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWcKKnT0elCp"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7YonhcKelCp"
      },
      "source": [
        "Optimizing the Model Parameters\n",
        "===============================\n",
        "\n",
        "To train a model, we need a [loss\n",
        "function](https://pytorch.org/docs/stable/nn.html#loss-functions) and an\n",
        "[optimizer](https://pytorch.org/docs/stable/optim.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CtZjRrBxelCp"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V2RZ7WYelCp"
      },
      "source": [
        "In a single training loop, the model makes predictions on the training\n",
        "dataset (fed to it in batches), and backpropagates the prediction error\n",
        "to adjust the model\\'s parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xstgAyygelCp"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4AfVnEselCp"
      },
      "source": [
        "We also check the model\\'s performance against the test dataset to\n",
        "ensure it is learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ld27u6RDelCp"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pizzeT3SelCq"
      },
      "source": [
        "The training process is conducted over several iterations (*epochs*).\n",
        "During each epoch, the model learns parameters to make better\n",
        "predictions. We print the model\\'s accuracy and loss at each epoch;\n",
        "we\\'d like to see the accuracy increase and the loss decrease with every\n",
        "epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U_nL_XaqelCq",
        "outputId": "7aad6fe5-9b12-411c-f4c1-53a48a831d1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.294626  [   64/60000]\n",
            "loss: 2.284598  [ 6464/60000]\n",
            "loss: 2.267783  [12864/60000]\n",
            "loss: 2.268368  [19264/60000]\n",
            "loss: 2.240323  [25664/60000]\n",
            "loss: 2.211937  [32064/60000]\n",
            "loss: 2.218487  [38464/60000]\n",
            "loss: 2.180247  [44864/60000]\n",
            "loss: 2.179112  [51264/60000]\n",
            "loss: 2.144003  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 38.4%, Avg loss: 2.143309 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.149746  [   64/60000]\n",
            "loss: 2.136794  [ 6464/60000]\n",
            "loss: 2.080144  [12864/60000]\n",
            "loss: 2.096886  [19264/60000]\n",
            "loss: 2.026935  [25664/60000]\n",
            "loss: 1.971640  [32064/60000]\n",
            "loss: 1.992816  [38464/60000]\n",
            "loss: 1.910562  [44864/60000]\n",
            "loss: 1.918794  [51264/60000]\n",
            "loss: 1.836058  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 52.3%, Avg loss: 1.845471 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.882217  [   64/60000]\n",
            "loss: 1.843659  [ 6464/60000]\n",
            "loss: 1.732669  [12864/60000]\n",
            "loss: 1.771758  [19264/60000]\n",
            "loss: 1.648032  [25664/60000]\n",
            "loss: 1.618205  [32064/60000]\n",
            "loss: 1.630078  [38464/60000]\n",
            "loss: 1.546562  [44864/60000]\n",
            "loss: 1.574218  [51264/60000]\n",
            "loss: 1.466808  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 60.4%, Avg loss: 1.492943 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.565764  [   64/60000]\n",
            "loss: 1.525440  [ 6464/60000]\n",
            "loss: 1.386471  [12864/60000]\n",
            "loss: 1.449183  [19264/60000]\n",
            "loss: 1.331547  [25664/60000]\n",
            "loss: 1.339184  [32064/60000]\n",
            "loss: 1.343923  [38464/60000]\n",
            "loss: 1.282336  [44864/60000]\n",
            "loss: 1.316942  [51264/60000]\n",
            "loss: 1.219853  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.5%, Avg loss: 1.246243 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.326315  [   64/60000]\n",
            "loss: 1.304330  [ 6464/60000]\n",
            "loss: 1.146459  [12864/60000]\n",
            "loss: 1.242947  [19264/60000]\n",
            "loss: 1.124286  [25664/60000]\n",
            "loss: 1.150818  [32064/60000]\n",
            "loss: 1.165118  [38464/60000]\n",
            "loss: 1.111678  [44864/60000]\n",
            "loss: 1.152186  [51264/60000]\n",
            "loss: 1.070660  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.5%, Avg loss: 1.089782 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.161515  [   64/60000]\n",
            "loss: 1.160655  [ 6464/60000]\n",
            "loss: 0.986666  [12864/60000]\n",
            "loss: 1.112581  [19264/60000]\n",
            "loss: 0.995221  [25664/60000]\n",
            "loss: 1.022092  [32064/60000]\n",
            "loss: 1.052545  [38464/60000]\n",
            "loss: 1.001261  [44864/60000]\n",
            "loss: 1.042635  [51264/60000]\n",
            "loss: 0.976285  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.7%, Avg loss: 0.987316 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.044495  [   64/60000]\n",
            "loss: 1.065701  [ 6464/60000]\n",
            "loss: 0.876383  [12864/60000]\n",
            "loss: 1.025127  [19264/60000]\n",
            "loss: 0.912873  [25664/60000]\n",
            "loss: 0.930404  [32064/60000]\n",
            "loss: 0.977659  [38464/60000]\n",
            "loss: 0.928403  [44864/60000]\n",
            "loss: 0.965456  [51264/60000]\n",
            "loss: 0.912355  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.2%, Avg loss: 0.916347 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.957573  [   64/60000]\n",
            "loss: 0.999016  [ 6464/60000]\n",
            "loss: 0.796750  [12864/60000]\n",
            "loss: 0.962451  [19264/60000]\n",
            "loss: 0.856831  [25664/60000]\n",
            "loss: 0.862363  [32064/60000]\n",
            "loss: 0.924231  [38464/60000]\n",
            "loss: 0.879262  [44864/60000]\n",
            "loss: 0.908537  [51264/60000]\n",
            "loss: 0.865728  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 68.4%, Avg loss: 0.864478 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.890786  [   64/60000]\n",
            "loss: 0.949109  [ 6464/60000]\n",
            "loss: 0.736783  [12864/60000]\n",
            "loss: 0.915002  [19264/60000]\n",
            "loss: 0.816033  [25664/60000]\n",
            "loss: 0.810481  [32064/60000]\n",
            "loss: 0.883546  [38464/60000]\n",
            "loss: 0.844914  [44864/60000]\n",
            "loss: 0.865261  [51264/60000]\n",
            "loss: 0.829632  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.5%, Avg loss: 0.824780 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.837564  [   64/60000]\n",
            "loss: 0.909344  [ 6464/60000]\n",
            "loss: 0.689783  [12864/60000]\n",
            "loss: 0.877808  [19264/60000]\n",
            "loss: 0.784398  [25664/60000]\n",
            "loss: 0.770118  [32064/60000]\n",
            "loss: 0.850916  [38464/60000]\n",
            "loss: 0.819670  [44864/60000]\n",
            "loss: 0.831297  [51264/60000]\n",
            "loss: 0.800416  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.0%, Avg loss: 0.793108 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.793597  [   64/60000]\n",
            "loss: 0.875884  [ 6464/60000]\n",
            "loss: 0.651552  [12864/60000]\n",
            "loss: 0.847817  [19264/60000]\n",
            "loss: 0.758580  [25664/60000]\n",
            "loss: 0.737960  [32064/60000]\n",
            "loss: 0.823454  [38464/60000]\n",
            "loss: 0.799985  [44864/60000]\n",
            "loss: 0.803935  [51264/60000]\n",
            "loss: 0.775878  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 72.2%, Avg loss: 0.766856 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.756259  [   64/60000]\n",
            "loss: 0.846427  [ 6464/60000]\n",
            "loss: 0.619474  [12864/60000]\n",
            "loss: 0.822945  [19264/60000]\n",
            "loss: 0.736830  [25664/60000]\n",
            "loss: 0.711691  [32064/60000]\n",
            "loss: 0.799281  [38464/60000]\n",
            "loss: 0.783686  [44864/60000]\n",
            "loss: 0.781168  [51264/60000]\n",
            "loss: 0.754516  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.2%, Avg loss: 0.744340 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.723846  [   64/60000]\n",
            "loss: 0.819815  [ 6464/60000]\n",
            "loss: 0.592032  [12864/60000]\n",
            "loss: 0.801753  [19264/60000]\n",
            "loss: 0.718006  [25664/60000]\n",
            "loss: 0.689835  [32064/60000]\n",
            "loss: 0.777406  [38464/60000]\n",
            "loss: 0.769483  [44864/60000]\n",
            "loss: 0.761597  [51264/60000]\n",
            "loss: 0.735313  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.0%, Avg loss: 0.724451 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.695268  [   64/60000]\n",
            "loss: 0.795394  [ 6464/60000]\n",
            "loss: 0.568052  [12864/60000]\n",
            "loss: 0.783105  [19264/60000]\n",
            "loss: 0.701326  [25664/60000]\n",
            "loss: 0.671294  [32064/60000]\n",
            "loss: 0.757202  [38464/60000]\n",
            "loss: 0.756667  [44864/60000]\n",
            "loss: 0.744548  [51264/60000]\n",
            "loss: 0.717682  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 74.8%, Avg loss: 0.706477 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.669724  [   64/60000]\n",
            "loss: 0.772709  [ 6464/60000]\n",
            "loss: 0.546768  [12864/60000]\n",
            "loss: 0.766460  [19264/60000]\n",
            "loss: 0.686620  [25664/60000]\n",
            "loss: 0.655412  [32064/60000]\n",
            "loss: 0.738254  [38464/60000]\n",
            "loss: 0.744750  [44864/60000]\n",
            "loss: 0.729459  [51264/60000]\n",
            "loss: 0.701454  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 75.4%, Avg loss: 0.690041 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.646719  [   64/60000]\n",
            "loss: 0.751668  [ 6464/60000]\n",
            "loss: 0.527800  [12864/60000]\n",
            "loss: 0.751310  [19264/60000]\n",
            "loss: 0.673645  [25664/60000]\n",
            "loss: 0.641666  [32064/60000]\n",
            "loss: 0.720320  [38464/60000]\n",
            "loss: 0.733754  [44864/60000]\n",
            "loss: 0.716200  [51264/60000]\n",
            "loss: 0.686440  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.2%, Avg loss: 0.674939 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.625898  [   64/60000]\n",
            "loss: 0.731905  [ 6464/60000]\n",
            "loss: 0.510756  [12864/60000]\n",
            "loss: 0.737353  [19264/60000]\n",
            "loss: 0.662012  [25664/60000]\n",
            "loss: 0.629601  [32064/60000]\n",
            "loss: 0.703362  [38464/60000]\n",
            "loss: 0.723776  [44864/60000]\n",
            "loss: 0.704315  [51264/60000]\n",
            "loss: 0.672307  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.8%, Avg loss: 0.660969 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.606978  [   64/60000]\n",
            "loss: 0.713423  [ 6464/60000]\n",
            "loss: 0.495278  [12864/60000]\n",
            "loss: 0.724419  [19264/60000]\n",
            "loss: 0.651488  [25664/60000]\n",
            "loss: 0.618819  [32064/60000]\n",
            "loss: 0.687373  [38464/60000]\n",
            "loss: 0.714620  [44864/60000]\n",
            "loss: 0.693858  [51264/60000]\n",
            "loss: 0.658949  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.648010 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.589645  [   64/60000]\n",
            "loss: 0.696290  [ 6464/60000]\n",
            "loss: 0.481164  [12864/60000]\n",
            "loss: 0.712433  [19264/60000]\n",
            "loss: 0.641991  [25664/60000]\n",
            "loss: 0.609313  [32064/60000]\n",
            "loss: 0.672395  [38464/60000]\n",
            "loss: 0.706468  [44864/60000]\n",
            "loss: 0.684775  [51264/60000]\n",
            "loss: 0.646483  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.9%, Avg loss: 0.636004 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.573869  [   64/60000]\n",
            "loss: 0.680438  [ 6464/60000]\n",
            "loss: 0.468306  [12864/60000]\n",
            "loss: 0.701264  [19264/60000]\n",
            "loss: 0.633227  [25664/60000]\n",
            "loss: 0.600817  [32064/60000]\n",
            "loss: 0.658386  [38464/60000]\n",
            "loss: 0.699239  [44864/60000]\n",
            "loss: 0.676955  [51264/60000]\n",
            "loss: 0.634807  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.3%, Avg loss: 0.624891 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.559386  [   64/60000]\n",
            "loss: 0.665764  [ 6464/60000]\n",
            "loss: 0.456501  [12864/60000]\n",
            "loss: 0.690904  [19264/60000]\n",
            "loss: 0.625172  [25664/60000]\n",
            "loss: 0.593136  [32064/60000]\n",
            "loss: 0.645336  [38464/60000]\n",
            "loss: 0.692932  [44864/60000]\n",
            "loss: 0.670174  [51264/60000]\n",
            "loss: 0.623870  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.5%, Avg loss: 0.614605 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.546063  [   64/60000]\n",
            "loss: 0.652166  [ 6464/60000]\n",
            "loss: 0.445646  [12864/60000]\n",
            "loss: 0.681189  [19264/60000]\n",
            "loss: 0.617725  [25664/60000]\n",
            "loss: 0.586108  [32064/60000]\n",
            "loss: 0.633182  [38464/60000]\n",
            "loss: 0.687605  [44864/60000]\n",
            "loss: 0.664396  [51264/60000]\n",
            "loss: 0.613559  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.9%, Avg loss: 0.605086 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.533730  [   64/60000]\n",
            "loss: 0.639579  [ 6464/60000]\n",
            "loss: 0.435684  [12864/60000]\n",
            "loss: 0.672009  [19264/60000]\n",
            "loss: 0.610635  [25664/60000]\n",
            "loss: 0.579619  [32064/60000]\n",
            "loss: 0.621810  [38464/60000]\n",
            "loss: 0.683040  [44864/60000]\n",
            "loss: 0.659423  [51264/60000]\n",
            "loss: 0.603692  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.1%, Avg loss: 0.596278 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.522333  [   64/60000]\n",
            "loss: 0.627943  [ 6464/60000]\n",
            "loss: 0.426487  [12864/60000]\n",
            "loss: 0.663372  [19264/60000]\n",
            "loss: 0.603903  [25664/60000]\n",
            "loss: 0.573579  [32064/60000]\n",
            "loss: 0.611227  [38464/60000]\n",
            "loss: 0.679137  [44864/60000]\n",
            "loss: 0.655144  [51264/60000]\n",
            "loss: 0.594207  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.6%, Avg loss: 0.588119 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.511682  [   64/60000]\n",
            "loss: 0.617150  [ 6464/60000]\n",
            "loss: 0.417919  [12864/60000]\n",
            "loss: 0.655153  [19264/60000]\n",
            "loss: 0.597361  [25664/60000]\n",
            "loss: 0.567909  [32064/60000]\n",
            "loss: 0.601441  [38464/60000]\n",
            "loss: 0.675962  [44864/60000]\n",
            "loss: 0.651575  [51264/60000]\n",
            "loss: 0.585185  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.7%, Avg loss: 0.580562 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.501707  [   64/60000]\n",
            "loss: 0.607116  [ 6464/60000]\n",
            "loss: 0.409970  [12864/60000]\n",
            "loss: 0.647408  [19264/60000]\n",
            "loss: 0.590942  [25664/60000]\n",
            "loss: 0.562492  [32064/60000]\n",
            "loss: 0.592357  [38464/60000]\n",
            "loss: 0.673458  [44864/60000]\n",
            "loss: 0.648491  [51264/60000]\n",
            "loss: 0.576516  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.0%, Avg loss: 0.573554 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.492385  [   64/60000]\n",
            "loss: 0.597797  [ 6464/60000]\n",
            "loss: 0.402576  [12864/60000]\n",
            "loss: 0.640002  [19264/60000]\n",
            "loss: 0.584576  [25664/60000]\n",
            "loss: 0.557232  [32064/60000]\n",
            "loss: 0.583835  [38464/60000]\n",
            "loss: 0.671481  [44864/60000]\n",
            "loss: 0.645863  [51264/60000]\n",
            "loss: 0.568096  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.1%, Avg loss: 0.567040 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.483646  [   64/60000]\n",
            "loss: 0.589135  [ 6464/60000]\n",
            "loss: 0.395564  [12864/60000]\n",
            "loss: 0.632928  [19264/60000]\n",
            "loss: 0.578224  [25664/60000]\n",
            "loss: 0.552125  [32064/60000]\n",
            "loss: 0.575873  [38464/60000]\n",
            "loss: 0.669916  [44864/60000]\n",
            "loss: 0.643621  [51264/60000]\n",
            "loss: 0.559934  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.4%, Avg loss: 0.560994 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.475416  [   64/60000]\n",
            "loss: 0.581125  [ 6464/60000]\n",
            "loss: 0.389015  [12864/60000]\n",
            "loss: 0.626160  [19264/60000]\n",
            "loss: 0.571956  [25664/60000]\n",
            "loss: 0.547200  [32064/60000]\n",
            "loss: 0.568488  [38464/60000]\n",
            "loss: 0.668790  [44864/60000]\n",
            "loss: 0.641647  [51264/60000]\n",
            "loss: 0.552054  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.6%, Avg loss: 0.555370 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.467620  [   64/60000]\n",
            "loss: 0.573699  [ 6464/60000]\n",
            "loss: 0.382940  [12864/60000]\n",
            "loss: 0.619703  [19264/60000]\n",
            "loss: 0.565819  [25664/60000]\n",
            "loss: 0.542284  [32064/60000]\n",
            "loss: 0.561659  [38464/60000]\n",
            "loss: 0.668084  [44864/60000]\n",
            "loss: 0.639906  [51264/60000]\n",
            "loss: 0.544412  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.7%, Avg loss: 0.550131 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.460223  [   64/60000]\n",
            "loss: 0.566791  [ 6464/60000]\n",
            "loss: 0.377261  [12864/60000]\n",
            "loss: 0.613549  [19264/60000]\n",
            "loss: 0.559720  [25664/60000]\n",
            "loss: 0.537456  [32064/60000]\n",
            "loss: 0.555321  [38464/60000]\n",
            "loss: 0.667642  [44864/60000]\n",
            "loss: 0.638330  [51264/60000]\n",
            "loss: 0.537033  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 80.9%, Avg loss: 0.545247 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.453179  [   64/60000]\n",
            "loss: 0.560374  [ 6464/60000]\n",
            "loss: 0.371917  [12864/60000]\n",
            "loss: 0.607639  [19264/60000]\n",
            "loss: 0.553711  [25664/60000]\n",
            "loss: 0.532698  [32064/60000]\n",
            "loss: 0.549435  [38464/60000]\n",
            "loss: 0.667394  [44864/60000]\n",
            "loss: 0.636892  [51264/60000]\n",
            "loss: 0.529895  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.0%, Avg loss: 0.540685 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.446509  [   64/60000]\n",
            "loss: 0.554348  [ 6464/60000]\n",
            "loss: 0.366935  [12864/60000]\n",
            "loss: 0.601992  [19264/60000]\n",
            "loss: 0.547815  [25664/60000]\n",
            "loss: 0.528030  [32064/60000]\n",
            "loss: 0.543921  [38464/60000]\n",
            "loss: 0.667228  [44864/60000]\n",
            "loss: 0.635532  [51264/60000]\n",
            "loss: 0.523005  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.2%, Avg loss: 0.536412 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.440210  [   64/60000]\n",
            "loss: 0.548802  [ 6464/60000]\n",
            "loss: 0.362265  [12864/60000]\n",
            "loss: 0.596594  [19264/60000]\n",
            "loss: 0.541982  [25664/60000]\n",
            "loss: 0.523428  [32064/60000]\n",
            "loss: 0.538757  [38464/60000]\n",
            "loss: 0.667107  [44864/60000]\n",
            "loss: 0.634180  [51264/60000]\n",
            "loss: 0.516366  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.3%, Avg loss: 0.532400 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.434207  [   64/60000]\n",
            "loss: 0.543635  [ 6464/60000]\n",
            "loss: 0.357902  [12864/60000]\n",
            "loss: 0.591413  [19264/60000]\n",
            "loss: 0.536298  [25664/60000]\n",
            "loss: 0.518905  [32064/60000]\n",
            "loss: 0.533910  [38464/60000]\n",
            "loss: 0.667013  [44864/60000]\n",
            "loss: 0.632883  [51264/60000]\n",
            "loss: 0.510032  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.528628 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.428460  [   64/60000]\n",
            "loss: 0.538826  [ 6464/60000]\n",
            "loss: 0.353797  [12864/60000]\n",
            "loss: 0.586424  [19264/60000]\n",
            "loss: 0.530737  [25664/60000]\n",
            "loss: 0.514431  [32064/60000]\n",
            "loss: 0.529358  [38464/60000]\n",
            "loss: 0.666909  [44864/60000]\n",
            "loss: 0.631588  [51264/60000]\n",
            "loss: 0.503916  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.525074 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.422952  [   64/60000]\n",
            "loss: 0.534301  [ 6464/60000]\n",
            "loss: 0.349935  [12864/60000]\n",
            "loss: 0.581571  [19264/60000]\n",
            "loss: 0.525287  [25664/60000]\n",
            "loss: 0.510060  [32064/60000]\n",
            "loss: 0.525057  [38464/60000]\n",
            "loss: 0.666737  [44864/60000]\n",
            "loss: 0.630256  [51264/60000]\n",
            "loss: 0.497999  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.7%, Avg loss: 0.521713 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.417660  [   64/60000]\n",
            "loss: 0.530082  [ 6464/60000]\n",
            "loss: 0.346260  [12864/60000]\n",
            "loss: 0.576912  [19264/60000]\n",
            "loss: 0.519934  [25664/60000]\n",
            "loss: 0.505801  [32064/60000]\n",
            "loss: 0.520985  [38464/60000]\n",
            "loss: 0.666509  [44864/60000]\n",
            "loss: 0.628919  [51264/60000]\n",
            "loss: 0.492333  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.518531 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.412558  [   64/60000]\n",
            "loss: 0.526119  [ 6464/60000]\n",
            "loss: 0.342760  [12864/60000]\n",
            "loss: 0.572409  [19264/60000]\n",
            "loss: 0.514744  [25664/60000]\n",
            "loss: 0.501633  [32064/60000]\n",
            "loss: 0.517139  [38464/60000]\n",
            "loss: 0.666188  [44864/60000]\n",
            "loss: 0.627564  [51264/60000]\n",
            "loss: 0.486910  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.515515 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.407654  [   64/60000]\n",
            "loss: 0.522403  [ 6464/60000]\n",
            "loss: 0.339434  [12864/60000]\n",
            "loss: 0.568093  [19264/60000]\n",
            "loss: 0.509756  [25664/60000]\n",
            "loss: 0.497548  [32064/60000]\n",
            "loss: 0.513488  [38464/60000]\n",
            "loss: 0.665769  [44864/60000]\n",
            "loss: 0.626192  [51264/60000]\n",
            "loss: 0.481785  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.9%, Avg loss: 0.512650 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.402919  [   64/60000]\n",
            "loss: 0.518913  [ 6464/60000]\n",
            "loss: 0.336296  [12864/60000]\n",
            "loss: 0.563942  [19264/60000]\n",
            "loss: 0.504943  [25664/60000]\n",
            "loss: 0.493622  [32064/60000]\n",
            "loss: 0.510012  [38464/60000]\n",
            "loss: 0.665238  [44864/60000]\n",
            "loss: 0.624829  [51264/60000]\n",
            "loss: 0.476889  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.509922 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.398358  [   64/60000]\n",
            "loss: 0.515636  [ 6464/60000]\n",
            "loss: 0.333342  [12864/60000]\n",
            "loss: 0.559919  [19264/60000]\n",
            "loss: 0.500273  [25664/60000]\n",
            "loss: 0.489764  [32064/60000]\n",
            "loss: 0.506671  [38464/60000]\n",
            "loss: 0.664538  [44864/60000]\n",
            "loss: 0.623406  [51264/60000]\n",
            "loss: 0.472241  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.507320 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.393947  [   64/60000]\n",
            "loss: 0.512535  [ 6464/60000]\n",
            "loss: 0.330513  [12864/60000]\n",
            "loss: 0.556081  [19264/60000]\n",
            "loss: 0.495723  [25664/60000]\n",
            "loss: 0.486001  [32064/60000]\n",
            "loss: 0.503455  [38464/60000]\n",
            "loss: 0.663712  [44864/60000]\n",
            "loss: 0.621980  [51264/60000]\n",
            "loss: 0.467875  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.1%, Avg loss: 0.504831 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.389676  [   64/60000]\n",
            "loss: 0.509605  [ 6464/60000]\n",
            "loss: 0.327764  [12864/60000]\n",
            "loss: 0.552349  [19264/60000]\n",
            "loss: 0.491370  [25664/60000]\n",
            "loss: 0.482375  [32064/60000]\n",
            "loss: 0.500349  [38464/60000]\n",
            "loss: 0.662794  [44864/60000]\n",
            "loss: 0.620534  [51264/60000]\n",
            "loss: 0.463647  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.2%, Avg loss: 0.502445 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.385521  [   64/60000]\n",
            "loss: 0.506866  [ 6464/60000]\n",
            "loss: 0.325124  [12864/60000]\n",
            "loss: 0.548780  [19264/60000]\n",
            "loss: 0.487154  [25664/60000]\n",
            "loss: 0.478904  [32064/60000]\n",
            "loss: 0.497402  [38464/60000]\n",
            "loss: 0.661762  [44864/60000]\n",
            "loss: 0.619075  [51264/60000]\n",
            "loss: 0.459657  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.500156 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.381490  [   64/60000]\n",
            "loss: 0.504302  [ 6464/60000]\n",
            "loss: 0.322635  [12864/60000]\n",
            "loss: 0.545334  [19264/60000]\n",
            "loss: 0.483056  [25664/60000]\n",
            "loss: 0.475533  [32064/60000]\n",
            "loss: 0.494544  [38464/60000]\n",
            "loss: 0.660666  [44864/60000]\n",
            "loss: 0.617494  [51264/60000]\n",
            "loss: 0.455883  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.3%, Avg loss: 0.497957 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.377576  [   64/60000]\n",
            "loss: 0.501805  [ 6464/60000]\n",
            "loss: 0.320237  [12864/60000]\n",
            "loss: 0.542023  [19264/60000]\n",
            "loss: 0.479066  [25664/60000]\n",
            "loss: 0.472310  [32064/60000]\n",
            "loss: 0.491748  [38464/60000]\n",
            "loss: 0.659566  [44864/60000]\n",
            "loss: 0.615882  [51264/60000]\n",
            "loss: 0.452308  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.4%, Avg loss: 0.495844 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.373782  [   64/60000]\n",
            "loss: 0.499361  [ 6464/60000]\n",
            "loss: 0.317967  [12864/60000]\n",
            "loss: 0.538825  [19264/60000]\n",
            "loss: 0.475221  [25664/60000]\n",
            "loss: 0.469228  [32064/60000]\n",
            "loss: 0.489000  [38464/60000]\n",
            "loss: 0.658429  [44864/60000]\n",
            "loss: 0.614258  [51264/60000]\n",
            "loss: 0.448910  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.5%, Avg loss: 0.493805 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.370039  [   64/60000]\n",
            "loss: 0.497023  [ 6464/60000]\n",
            "loss: 0.315735  [12864/60000]\n",
            "loss: 0.535761  [19264/60000]\n",
            "loss: 0.471599  [25664/60000]\n",
            "loss: 0.466269  [32064/60000]\n",
            "loss: 0.486358  [38464/60000]\n",
            "loss: 0.657179  [44864/60000]\n",
            "loss: 0.612596  [51264/60000]\n",
            "loss: 0.445654  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.5%, Avg loss: 0.491840 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.366347  [   64/60000]\n",
            "loss: 0.494802  [ 6464/60000]\n",
            "loss: 0.313566  [12864/60000]\n",
            "loss: 0.532804  [19264/60000]\n",
            "loss: 0.468023  [25664/60000]\n",
            "loss: 0.463446  [32064/60000]\n",
            "loss: 0.483828  [38464/60000]\n",
            "loss: 0.655848  [44864/60000]\n",
            "loss: 0.610984  [51264/60000]\n",
            "loss: 0.442567  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.5%, Avg loss: 0.489936 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KacHAUHbelCq"
      },
      "source": [
        "Read more about [Training your model](optimization_tutorial.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrmwtVgrelCq"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO8m80ERelCq"
      },
      "source": [
        "Saving Models\n",
        "=============\n",
        "\n",
        "A common way to save a model is to serialize the internal state\n",
        "dictionary (containing the model parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "T3ULM_-delCq",
        "outputId": "b25f4a9c-4cd1-49bf-b3bf-027297d11daf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hpc1FNkJelCq"
      },
      "source": [
        "Loading Models\n",
        "==============\n",
        "\n",
        "The process for loading a model includes re-creating the model structure\n",
        "and loading the state dictionary into it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RiitV0MeelCq",
        "outputId": "221cebf6-0256-4df3-9e21-1e4e11ba1e4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHnLEz3MelCq"
      },
      "source": [
        "This model can now be used to make predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "r6s-7JolelCq",
        "outputId": "ee12f948-eb99-4ead-ac8f-dfa3c3696e3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
          ]
        }
      ],
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "    x = x.to(device)\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXNsnFX4elCr"
      },
      "source": [
        "Read more about [Saving & Loading your\n",
        "model](saveloadrun_tutorial.html).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}